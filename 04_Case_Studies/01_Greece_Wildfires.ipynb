{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "245011b7-9455-47cd-83a0-1495bb8984dd",
   "metadata": {},
   "source": [
    "# Greece Wildfires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4f1844-476f-4fd6-a5f3-fcb26b5c0ad2",
   "metadata": {},
   "source": [
    "In this example, we will retrieve data associated with the [2023 Greece wildfires](https://en.wikipedia.org/wiki/2023_Greece_wildfires) to understand their evolution and extent. We will generate a time series associated with this data and two visualizations of the event.\n",
    "\n",
    "In particular, we will examine the area around the city of [Alexandroupolis](https://en.wikipedia.org/wiki/Alexandroupolis) which was severely impacted by the wildfires, resulting in loss of lives, property, and forested areas.\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://www.cde.ual.es/wp-content/uploads/2023/07/forestfires_hp1.jpg\"></img>\n",
    "    (from https://www.cde.ual.es/en/forest-fires-in-greece-eu-mobilises-firefighting-assistance/)\n",
    "</center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea84af60-83ac-45f5-ae03-7c977c4af7cd",
   "metadata": {},
   "source": [
    "## Outline of steps for analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad3b870c-c6cd-4b8a-bedd-bea0f34b00dd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "+ Identifying search parameters\n",
    "    + AOI, time-window\n",
    "    + Endpoint, Provider, catalog identifier (\"short name\")\n",
    "+ Obtaining search results\n",
    "    + Instrospect, examine to identify features, bands of interest\n",
    "    + Wrap results into a DataFrame for easier exploration\n",
    "+ Exploring & refining search results\n",
    "    + Identify granules of highest value\n",
    "    + Filter extraneous granules with minimal contribution\n",
    "    + Assemble relevant filtered granules into DataFrame\n",
    "    + Identify kind of output to generate\n",
    "+ Data-wrangling to produce relevant output\n",
    "    + Download relevant granules into Xarray DataArray, stacked appropriately\n",
    "    + Do intermediate computations as necessary\n",
    "    + Assemble relevant data slices into visualization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9030e8-ec21-4ac7-8689-bf42cd23ac8b",
   "metadata": {},
   "source": [
    "### Preliminary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b238c2f-d5f1-45ec-9ecb-2a009fbc0159",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "import numpy as np, pandas as pd, xarray as xr\n",
    "import rioxarray as rio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a254228-a476-4afa-b604-8b987d745c34",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imports for plotting\n",
    "import hvplot.pandas, hvplot.xarray\n",
    "import geoviews as gv\n",
    "from geoviews import opts\n",
    "gv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810b451e-7913-418e-8885-0cd28ba04c02",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# STAC imports to retrieve cloud data\n",
    "from pystac_client import Client\n",
    "from osgeo import gdal\n",
    "# GDAL setup for accessing cloud data\n",
    "gdal.SetConfigOption('GDAL_HTTP_COOKIEFILE','~/.cookies.txt')\n",
    "gdal.SetConfigOption('GDAL_HTTP_COOKIEJAR', '~/.cookies.txt')\n",
    "gdal.SetConfigOption('GDAL_DISABLE_READDIR_ON_OPEN','EMPTY_DIR')\n",
    "gdal.SetConfigOption('CPL_VSIL_CURL_ALLOWED_EXTENSIONS','TIF, TIFF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9ec09a-4c34-4fd1-a563-fc7ff6988aae",
   "metadata": {},
   "source": [
    "### Convenient utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c0acb1-10ea-4157-8d5f-f8346823496a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# simple utility to make a rectangle with given center of width dx & height dy\n",
    "def make_bbox(pt,dx,dy):\n",
    "    '''Returns bounding-box represented as tuple (x_lo, y_lo, x_hi, y_hi)\n",
    "    given inputs pt=(x, y), width & height dx & dy respectively,\n",
    "    where x_lo = x-dx/2, x_hi=x+dx/2, y_lo = y-dy/2, y_hi = y+dy/2.\n",
    "    '''\n",
    "    return tuple(coord+sgn*delta for sgn in (-1,+1) for coord,delta in zip(pt, (dx/2,dy/2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e80a32a-ad1f-4b2b-8feb-ac6ebce4dea8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# simple utility to plot an AOI or bounding-box\n",
    "def plot_bbox(bbox):\n",
    "    '''Given bounding-box, returns GeoViews plot of Rectangle & Point at center\n",
    "    + bbox: bounding-box specified as (lon_min, lat_min, lon_max, lat_max)\n",
    "    Assume longitude-latitude coordinates.\n",
    "    '''\n",
    "    # These plot options are fixed but can be over-ridden\n",
    "    point_opts = opts.Points(size=12, alpha=0.25, color='blue')\n",
    "    rect_opts = opts.Rectangles(line_width=2, alpha=0.1, color='red')\n",
    "    lon_lat = (0.5*sum(bbox[::2]), 0.5*sum(bbox[1::2]))\n",
    "    return (gv.Points([lon_lat]) * gv.Rectangles([bbox])).opts(point_opts, rect_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d876c-f625-40e6-ae47-d893b57f60b5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# utility to extract search results into a Pandas DataFrame\n",
    "def search_to_dataframe(search):\n",
    "    '''Constructs Pandas DataFrame from PySTAC Earthdata search results.\n",
    "    DataFrame columns are determined from search item properties and assets.\n",
    "    'asset': string identifying an Asset type associated with a granule\n",
    "    'href': data URL for file associated with the Asset in a given row.'''\n",
    "    granules = list(search.items())\n",
    "    assert granules, \"Error: empty list of search results\"\n",
    "    props = list({prop for g in granules for prop in g.properties.keys()})\n",
    "    tile_ids = map(lambda granule: granule.id.split('_')[3], granules)\n",
    "    rows = (([g.properties.get(k, None) for k in props] + [a, g.assets[a].href, t])\n",
    "                for g, t in zip(granules,tile_ids) for a in g.assets )\n",
    "    df = pd.concat(map(lambda x: pd.DataFrame(x, index=props+['asset','href', 'tile_id']).T, rows),\n",
    "                   axis=0, ignore_index=True)\n",
    "    assert len(df), \"Empty DataFrame\"\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa3f7c3a-45e4-4453-a739-80f6c7602a35",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "These functions could be placed in module files for more developed research projects. For learning purposes, they are embedded within this notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b67ef83-8c2e-4026-93ea-66f4942680b4",
   "metadata": {},
   "source": [
    "## Identifying search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad1c0ce-876d-415b-8c41-694a620caee7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dadia_forest = (26.18, 41.08)\n",
    "AOI = make_bbox(dadia_forest, 0.1, 0.1)\n",
    "DATE_RANGE = '2023-08-01/2023-09-30'.split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf723fd-cc11-45bf-802e-e975817559b6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Optionally plot the AOI\n",
    "basemap = gv.tile_sources.ESRI(width=500, height=500, padding=0.1, alpha=0.25)\n",
    "plot_bbox(AOI) * basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cab3f86-396e-4ac7-9a08-bea7539fd1fe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "search_params = dict(bbox=AOI, datetime=DATE_RANGE)\n",
    "print(search_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ddaf9c-277a-4abc-94f6-9297968f0872",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea987687-d1e8-4812-a997-51eca00f1ab1",
   "metadata": {},
   "source": [
    "## Obtaining search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70ab1ad-46d3-4517-abc9-fc52fbb3064e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ENDPOINT = 'https://cmr.earthdata.nasa.gov/stac'\n",
    "PROVIDER = 'LPCLOUD'\n",
    "COLLECTIONS = [\"OPERA_L3_DIST-ALERT-HLS_V1_1\"]\n",
    "# Update the dictionary opts with list of collections to search\n",
    "search_params.update(collections=COLLECTIONS)\n",
    "print(search_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe57ef-b2bd-474c-bb7f-e4127b665a66",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "catalog = Client.open(f'{ENDPOINT}/{PROVIDER}/')\n",
    "search_results = catalog.search(**search_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c916f3b3-162c-4e11-991f-24c6f12c3bf0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "As usual, let's encode the search result into a Pandas `DataFrame`, examine the results, and make a few transformations to clean up the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b53cc2b-80f5-4222-b9c8-98e7cd9faf5b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df = search_to_dataframe(search_results)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676169e6-06b1-42b3-868f-a57b890e4b5b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "We clean the `DataFrame` `df` in typical ways that make sense:\n",
    "+ casting the `datetime` column as `DatetimeIndex`;\n",
    "+ dropping extraneous `datetime` columns;\n",
    "+ renaming the `eo:cloud_cover` column as `cloud_cover`;\n",
    "+ casting the `cloud_cover` column as floating-point values; and\n",
    "+ casting the remaining columns as strings; and\n",
    "+ setting the `datetime` column as the `Index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8346ed5-18d2-4fb2-9238-5fe7b3f581d7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(['end_datetime', 'start_datetime'], axis=1)\n",
    "df.datetime = pd.DatetimeIndex(df.datetime)\n",
    "df = df.rename(columns={'eo:cloud_cover':'cloud_cover'})\n",
    "df['cloud_cover'] = df['cloud_cover'].astype(np.float16)\n",
    "for col in ['asset', 'href', 'tile_id']:\n",
    "    df[col] = df[col].astype(pd.StringDtype())\n",
    "df = df.set_index('datetime').sort_index()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddf054d-7e34-4114-a030-ca098baee8f4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f04677-4e12-4405-b3f9-6cb90382c4bf",
   "metadata": {},
   "source": [
    "## Exploring & refining search results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6301d5c-8b9d-4c1b-ac4c-90c80ac787df",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Let's examine the `DataFrame` `df` to better understand the search results. First, let's see how many different geographic tiles occur in the search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e008c1-00f0-46ec-8d80-3c04fa7fe873",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.tile_id.value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6907297-63e5-449c-804a-fca31031b11c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "So the AOI lies strictly within a single MGRS geographic tile, namely `T35TMF`. Let's examine the `asset` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec723a6-2987-498c-aec4-d1bb53387efd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.asset.value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2290421-fa61-4860-b603-ea661741f823",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Some of these `asset` names are not as simple and tidy as the ones we encountered with the DIST-ALERT data products. We can, however, easily identify useful substrings. Here, we choose only those rows in which the `asset` column includes `'VEG-DIST-STATUS'` as a sub-string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d174d85-7401-452b-bbe8-2cbcbec742ee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "idx_veg_dist_status = df.asset.str.contains('VEG-DIST-STATUS')\n",
    "idx_veg_dist_status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8438213-da18-4a7c-816f-224ed47a644d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "We can use this boolean `Series` with the Pandas `.loc` accessor to filter out only the rows we want (i.e., that connect to raster data files storing the `VEG-DIST-STATUS` band). We can subsequently drop the `asset` column (it is no longer required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbca0444-ccd7-462c-b3a2-1e3769661056",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "veg_dist_status = df.loc[idx_veg_dist_status]\n",
    "veg_dist_status = veg_dist_status.drop('asset', axis=1)\n",
    "veg_dist_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904acafa-8212-4b3b-a04a-b69a00181959",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(len(veg_dist_status))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a25181-5f9f-4bdb-833a-835e0821ad95",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Notice some of the rows have the same date but different times (corresponding to multiple observations in the same UTC calendar day). We can aggregate the URLs into lists by *resampling* the time series by day; we can subsequently visualize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de11c9a-cc02-471b-a4d6-2c25fa5d8475",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "by_day = veg_dist_status.resample('1d').href.apply(list)\n",
    "display(by_day)\n",
    "by_day.map(len).hvplot.scatter(grid=True).opts(title='# of observations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70bcd30-08cf-41b5-957b-206df8a5b290",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Let's clean up the `Series` `by_day` by filtering out the rows that have empty lists (i.e., dates on which no data was acquired)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17a19b0-9700-4f70-903e-7a74b61eeb3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "by_day = by_day.loc[by_day.map(bool)]\n",
    "by_day.map(len).hvplot.scatter(ylim=(0,2.1), grid=True).opts(title=\"# of observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d837d117-7869-4206-ab5e-4b480572ef13",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "We can now use the resampled series `by_day` to extract raster data for analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a93a1e2-d4e1-473b-ba9e-c467cf92b792",
   "metadata": {},
   "source": [
    "## Data-wrangling to produce relevant output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9265efd5-4abb-49f6-a544-d28ccd89f7d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The wildfire near Alexandroupolis started around August 21st and rapidly spread, particularly affecting the nearby Dadia Forest. Let's first assemble a \"data cube\" (i.e., a stacked array of rasters) from the remote files indexed in the Pandas series `by_day`. We'll start by selecting and loading one of the remote GeoTIFF files to extract metadata that applies to all the rasters associated with this event and this MGRS tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aabd459-ef8e-47a1-8659-6abb7cc0e6f6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "href = by_day[0][0]\n",
    "data = rio.open_rasterio(href).rename(dict(x='longitude', y='latitude'))\n",
    "crs = data.rio.crs\n",
    "shape = data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397d8fb9-be02-4ec8-8488-511dc6fbc500",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Before we build a stacked `DataArray` within a loop, we'll define a Python dictionary called  `template` that will be used to instantiate the slices of the array. The dictionary `template` will store metadata extracted from the GeoTIFF file, notably the coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b68ca6d-cea1-4472-9474-49f4a24327a3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "template = dict()\n",
    "template['coords'] = data.coords.copy()\n",
    "del template['coords']['band']\n",
    "template['coords'].update({'time': by_day.index.values})\n",
    "template['dims'] = ['time', 'longitude', 'latitude']\n",
    "template['attrs'] = dict(description=f\"OPERA DSWX: VEG-DIST-STATUS\", units=None)\n",
    "\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80db69cc-d3f0-4f41-84dd-ad1b3376aa86",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "We'll use a loop to build a stacked array of rasters from the Pandas series `by_day` (whose entries are lists of string, i.e., URIs). If the list has a singleton element, the URI can be read directly using `rasterio.open`; otherwise, the function [`rasterio.merge.merge`](https://rasterio.readthedocs.io/en/latest/api/rasterio.merge.html) combines multiple raster data files acquired on the same day into a single raster image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2760c0e-2ec9-43d9-bcb9-021bfaad3985",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.merge import merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da26db1-b440-498c-a7ab-34323dbc1d71",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rasters = []\n",
    "for date, hrefs in by_day.items():\n",
    "    n_files = len(hrefs)\n",
    "    if n_files > 1:\n",
    "        print(f\"Merging {n_files} files for {date.strftime('%Y-%m-%d')}...\")\n",
    "        raster, _ = merge(hrefs)\n",
    "    else:\n",
    "        print(f\"Opening {n_files} file  for {date.strftime('%Y-%m-%d')}...\")\n",
    "        with rasterio.open(hrefs[0]) as ds:\n",
    "            raster = ds.read()\n",
    "    rasters.append(np.reshape(raster, newshape=shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30bf613-7183-47f0-8b57-bfa916304f34",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The data accumulated within the list `rasters` are all stored as NumPy arrays. Thus, rather than calling `xarray.concat`, we wrap a call to `numpy.concatenate` within a call to the `xarray.DataArray` constructor. We bind the object created to the identifier `stack`, making sure to also include the CRS information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1cc633-6274-4f83-a82c-7263461b9d37",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "stack = xr.DataArray(data=np.concatenate(rasters, axis=0), **template)\n",
    "stack.rio.write_crs(crs, inplace=True)\n",
    "stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14277134-2f7d-497f-852b-6bfef051e752",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The `DataArray` `stack` has `time`, `longitude`, and `latitude` as its main coordinate dimensions. We can use this to perform some computations and produce relevant visualizations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feea99d-5157-4f94-b6d2-53d89adad2e3",
   "metadata": {},
   "source": [
    "### Plotting the area damaged"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e0bc55b-f62b-4a26-9a56-232a965b0074",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "To begin, let's use the data in `stack` to compute the total surface area damaged. The data in `stack` comes from `VEG-DIST-STATUS`  band of the DIST-ALERT product. We interpret the pixel values in this band as follows:\n",
    "\n",
    "* **0:** No disturbance\n",
    "* **1:** First detection of disturbance with vegetation cover change $<50\\%$\n",
    "* **2:** Provisional detection of disturbance with vegetation cover change $<50\\%$\n",
    "* **3:** Confirmed detection of disturbance with vegetation cover change $<50\\%$\n",
    "* **4:** First detection of disturbance with vegetation cover change $\\ge50\\%$\n",
    "* **5:** Provisional detection of disturbance with vegetation cover change $\\ge50\\%$\n",
    "* **6:** Confirmed detection of disturbance with vegetation cover change $\\ge50\\%$\n",
    "* **7:** Finished detection of disturbance with vegetation cover change $<50\\%$\n",
    "* **8:** Finished detection of disturbance with vegetation cover change $\\ge50\\%$\n",
    "\n",
    "The particular pixel value we want to flag, then, is 6, i.e., a pixel in which at least 50% of the vegetation cover has been confirmed to be damaged and in which the disturbance is actively ongoing. We can use the `.sum` method to add up all the pixels with value `6` and the `.to_series` method to represent the sum as a time-indexed Pandas series. We also define `conversion_factor` that accounts for the area of each pixel in $\\mathrm{km}^2$ (recall each pixel has area $30\\mathrm{m}\\times30\\mathrm{m}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a82d3cf-7ea3-4537-9e47-6e67d60f7035",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pixel_val = 6\n",
    "conversion_factor = (30/1_000)**2 / pixel_val\n",
    "damage_area = stack.where(stack==pixel_val, other=0).sum(axis=(1,2)) \n",
    "damage_area = damage_area.to_series() * conversion_factor\n",
    "damage_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc40b39-bf8d-4bd0-8f97-1aee0b679d4f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plot_title = 'Damaged Area (kmÂ²)'\n",
    "line_plot_opts = dict(title=plot_title, grid=True, color='r')\n",
    "damage_area.hvplot.line(**line_plot_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804f38c5-facb-492f-a75a-9ed97b51cf4d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Looking at the preceding plot, it seems the wildfires started around August 21 and spread rapidly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53625b14-3384-44b0-945f-4a8c554ccf39",
   "metadata": {},
   "source": [
    "### Viewing selected time slices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaa63df-efbb-4664-bb6b-a563c76060bb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The nearby Dadia Forest was particularly affected by the wildfires. To see this, let's plot the raster data to see the spatial distribution of damaged pixels on three particular dates:  August 2, August 26th, and September 18th. Again, we'll highlight only those pixels with value 6 from the raster data. We can extract those specific dates easily from the Time series `by_day` using a list of dates (i.e., `dates_of_interest` in the cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a4e82a-172a-4935-a227-a9d1a56042b3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dates_of_interest = ['2023-08-01', '2023-08-26', '2023-09-18']\n",
    "print(dates_of_interest)\n",
    "snapshots = stack.sel(time=dates_of_interest)\n",
    "snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbde38bd-2221-49a8-94c5-ea1a0c0b236c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Let's make a static sequence of plots. We start by defining some standard options stored in dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa27d7d2-fa2e-4aee-ac87-7906c7f1f0e8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "image_opts = dict(\n",
    "                    x='longitude', \n",
    "                    y='latitude',\n",
    "                    rasterize=True, \n",
    "                    dynamic=True,\n",
    "                    crs=crs,\n",
    "                    shared_axes=False,\n",
    "                    colorbar=False,\n",
    "                    aspect='equal',\n",
    "                 )\n",
    "layout_opts = dict(xlabel='Longitude', ylabel=\"Latitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a7b50a-049c-403c-a0e6-46e21dc7b7ae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "We'll construct a colormap using a dictionary of [RGBA (\"red green blue alpha\")](https://en.wikipedia.org/wiki/RGBA_color_model) values (i.e., tuples with three integer entries between 0 and 255 and a fourth floating-point entry between 0.0 and 1.0 for transparency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7615008f-4f21-41fa-bf18-6fd68ff09e0a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "COLORS = { k:(0,0,0,0.0) for k in range(256) }\n",
    "COLORS.update({6: (255,0,0,1.0)})\n",
    "image_opts.update(cmap=list(COLORS.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab9d270-be81-431b-88c8-79d14d5f8ff1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "As usual, we'll start by slicing smaller images to make sure the call to `hvplot.image` works as intended. We can reduce the value of the parameter `steps` to `1` or `None` to get the images rendered at full resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b07ccc-a165-4ca5-a6f4-f09f46464f7e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "steps = 100\n",
    "subset = slice(0,None, steps)\n",
    "view = snapshots.isel(longitude=subset, latitude=subset)\n",
    "(view.hvplot.image(**image_opts).opts(**layout_opts) * basemap).layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41bee32-170d-4695-a03b-6afd11d1585f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "If we remove the call to `.layout`, we can produce an interactive slider that shows the progress of the wildfire using all the rasters in `stack`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4118eae1-b44f-4a51-bf97-d62e7f728700",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "steps = 100\n",
    "subset = slice(0,None, steps)\n",
    "view = stack.isel(longitude=subset, latitude=subset,)\n",
    "(view.hvplot.image(**image_opts).opts(**layout_opts) * basemap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8690726c-b788-427e-8cba-44733c5e2240",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
